{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from planet import api\n",
    "from planet.api import filters\n",
    "from sys import stdout\n",
    "import urllib.request\n",
    "import shutil\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import cv2\n",
    "import glob\n",
    "import subprocess\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import wikipedia\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your volcano of interest: Anak Krakatoa\n"
     ]
    }
   ],
   "source": [
    "title = input(\"Please input your volcano of interest: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wikipedia.WikipediaPage(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia summary for your volcano of interest: \n",
      "\n",
      "Anak Krakatoa, (English: Child of Krakatoa) is an island in a caldera in the Sunda Strait between the islands of Java and Sumatra in the Indonesian province of Lampung. On 29 December 1927, Anak Krakatau, emerged from the caldera formed in 1883 by the explosive volcanic eruption which destroyed the island of Krakatoa. There has been sporadic eruptive activity at the site since the late 20th century, culminating with a large underwater collapse of the volcano which caused a deadly tsunami in December 2018, followed by subsequent activity in 2019. Due to its young age, the island is one of several in the area which are of considerable interest to volcanologists, and the subject of extensive study.\n"
     ]
    }
   ],
   "source": [
    "print(\"Wikipedia summary for your volcano of interest: \")\n",
    "print(\"\")\n",
    "print(wikipedia.summary(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References from the wikipedia article of your volcano of interest: \n",
      "\n",
      "[ 1 ] : http://in.reuters.com/article/worldNews/idINIndia-30436520071111?sp=true\n",
      "[ 2 ] : http://portal.vsi.esdm.go.id/joomla/\n",
      "[ 3 ] : http://doi.org/10.1144%2FSP361.7\n",
      "[ 4 ] : http://sp.lyellcollection.org/content/361/1/79\n",
      "[ 5 ] : http://tools.wmflabs.org/geohack/geohack.php?pagename=Anak_Krakatoa&params=6.102_S_105.423_E_type:mountain_scale:100000\n",
      "[ 6 ] : http://www.worldcat.org/issn/0305-8719\n",
      "[ 7 ] : https://www.bbc.com/news/science-environment-46707731\n",
      "[ 8 ] : https://www.nbcnews.com/news/world/indonesia-s-anak-krakatau-volcano-triggered-deadly-tsunami-now-quarter-n952906\n",
      "[ 9 ] : https://www.rt.com/news/441477-tour-boat-fire-bombed-volcano/\n",
      "[ 10 ] : https://www.volcanodiscovery.com/krakatau/news/80657/Krakatau-volcano-Indonesia-activity-update-and-field-report-increasing-unrest.html\n",
      "[ 11 ] : https://www.volcanodiscovery.com/krakatoa/2018/dec/eruption-tsunami/updates.html\n",
      "[ 12 ] : https://volcano.si.edu/volcano.cfm?vn=262000\n",
      "[ 13 ] : https://www.thestar.com.my/news/regional/2018/12/31/number-of-injured-in-indonesia-tsunami-surges-to-over-14000/\n",
      "[ 14 ] : https://web.archive.org/web/20090506120112/http://portal.vsi.esdm.go.id/joomla/\n",
      "[ 15 ] : https://viaf.org/viaf/315128839\n",
      "[ 16 ] : https://www.wikidata.org/wiki/Q2205302\n",
      "[ 17 ] : https://www.worldcat.org/identities/containsVIAFID/315128839\n",
      "[ 18 ] : https://www.bbc.co.uk/news/science-environment-46825354\n",
      "[ 19 ] : https://www.telegraph.co.uk/news/2018/12/22/indonesia-tsunami-least-20-dead-165-injured-waves-hit-beaches/\n"
     ]
    }
   ],
   "source": [
    "print(\"References from the wikipedia article of your volcano of interest: \")\n",
    "print(\"\")\n",
    "\n",
    "References_Wikipedia = page.references\n",
    "\n",
    "len(References_Wikipedia)\n",
    "x = 1\n",
    "\n",
    "for i in References_Wikipedia:\n",
    "    print('[',x,'] :',i)\n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related wikipedia articles to your volcano of interest: \n",
      "\n",
      "[ 1 ] : Krakatoa archipelago - https://en.wikipedia.org/wiki/Krakatoa_archipelago\n",
      "[ 2 ] : Anak Krakatoa - https://en.wikipedia.org/wiki/Anak_Krakatoa\n",
      "[ 3 ] : Krakatoa - https://en.wikipedia.org/wiki/Krakatoa\n",
      "[ 4 ] : 1883 eruption of Krakatoa - https://en.wikipedia.org/wiki/1883_eruption_of_Krakatoa\n",
      "[ 5 ] : 2018 Sunda Strait tsunami - https://en.wikipedia.org/wiki/2018_Sunda_Strait_tsunami\n"
     ]
    }
   ],
   "source": [
    "print(\"Related wikipedia articles to your volcano of interest: \")\n",
    "print(\"\")\n",
    "\n",
    "Related_Wikipedia = wikipedia.geosearch(page.coordinates[0], page.coordinates[1])\n",
    "\n",
    "len(Related_Wikipedia)\n",
    "x = 1\n",
    "\n",
    "for i in Related_Wikipedia:\n",
    "    print ('[',x,'] :', i, '-', wikipedia.page(i).url)\n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = float(page.coordinates[0])\n",
    "long = float(page.coordinates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your start date (YYYY-MM-DD): 2018-12-01\n"
     ]
    }
   ],
   "source": [
    "start_date = input(\"Please input your start date (YYYY-MM-DD): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your end date (YYYY-MM-DD): 2019-01-31\n"
     ]
    }
   ],
   "source": [
    "end_date = input(\"Please input your end date (YYYY-MM-DD): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the percent of maximum cloud cover you want to allow in each image as an integer between 0 and 1 (e.g. 20% = 0.2): 0.2\n"
     ]
    }
   ],
   "source": [
    "cloud_cover = input(\"Please input the percent of maximum cloud cover you want to allow in each image as an integer between 0 and 1 (e.g. 20% = 0.2): \")\n",
    "cloud_cover = float(cloud_cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input you Planet API key: fa8bc840c2af4b52a4f6a69e3489fd10\n"
     ]
    }
   ],
   "source": [
    "api_key = input(\"Please input you Planet API key: \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = api.ClientV1(api_key) \n",
    "PL_API_KEY = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_json_geometry = {\n",
    "  \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              long - 0.07,\n",
    "              lat + 0.07\n",
    "            ],\n",
    "            [\n",
    "              long + 0.07,\n",
    "              lat + 0.07\n",
    "            ],\n",
    "            [\n",
    "              long + 0.07,\n",
    "              lat - 0.07\n",
    "            ],\n",
    "            [\n",
    "              long - 0.07,\n",
    "              lat - 0.07\n",
    "            ],\n",
    "            [\n",
    "              long - 0.07,\n",
    "              lat + 0.07\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_filter = {\n",
    "    \"type\": \"GeometryFilter\",\n",
    "    \"field_name\": \"geometry\",\n",
    "    \"config\": geo_json_geometry\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range_filter = {\n",
    "    \"type\": \"DateRangeFilter\",\n",
    "    \"field_name\": \"acquired\",\n",
    "    \"config\": {\n",
    "        \"gte\": start_date + \"T00:00:00.000Z\",\n",
    "        \"lte\": end_date + \"T00:00:00.000Z\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_cover_filter = {\n",
    "    \"type\": \"RangeFilter\",\n",
    "    \"field_name\": \"cloud_cover\",\n",
    "    \"config\": {\n",
    "        \"lte\": cloud_cover,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_types = ['PSScene4Band']\n",
    "asset_type = 'analytic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"type\": \"AndFilter\",\n",
    "    \"config\": [geometry_filter, date_range_filter, cloud_cover_filter]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = filters.build_search_request(\n",
    "    query, item_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,cloud_cover,date\n"
     ]
    }
   ],
   "source": [
    "dataset = client.quick_search(request)\n",
    "\n",
    "stdout.write('id,cloud_cover,date\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190129_030555_1053,0.18,2019-01-29T03:05:55.637413Z\n",
      "20190102_031011_0f2d,0.2,2019-01-02T03:10:11.888407Z\n",
      "20190126_030500_1051,0.13,2019-01-26T03:05:00.817577Z\n",
      "20190128_025336_1034,0.13,2019-01-28T02:53:36.276178Z\n",
      "20190129_025214_101b,0.1,2019-01-29T02:52:14.358321Z\n",
      "20190124_025423_1040,0,2019-01-24T02:54:23.125827Z\n",
      "20190112_025426_1018,0.02,2019-01-12T02:54:26.28183Z\n",
      "20190112_025425_1018,0.01,2019-01-12T02:54:25.23273Z\n",
      "20190112_025424_1018,0,2019-01-12T02:54:24.18363Z\n",
      "20190113_025310_0f42,0.01,2019-01-13T02:53:10.998835Z\n",
      "20190113_025309_0f42,0.15,2019-01-13T02:53:09.924045Z\n",
      "20190107_025219_1027,0.14,2019-01-07T02:52:19.741871Z\n",
      "20190107_025218_1027,0.03,2019-01-07T02:52:18.69382Z\n",
      "20190107_025217_1027,0.07,2019-01-07T02:52:17.645768Z\n",
      "20190107_025220_1027,0.01,2019-01-07T02:52:20.789923Z\n",
      "20190106_030927_0f33,0.1,2019-01-06T03:09:27.609053Z\n",
      "20190104_025219_101b,0.06,2019-01-04T02:52:19.043151Z\n",
      "20190104_025217_101b,0.17,2019-01-04T02:52:17.993527Z\n",
      "20190104_025215_101b,0.13,2019-01-04T02:52:15.893229Z\n",
      "20190104_025344_0f43,0.07,2019-01-04T02:53:44.06877Z\n",
      "20190104_025343_0f43,0.09,2019-01-04T02:53:43.019145Z\n",
      "20181230_025232_102e,0.17,2018-12-30T02:52:32.968183Z\n",
      "20181230_025229_102e,0.14,2018-12-30T02:52:29.831892Z\n",
      "20181220_025049_0f35,0.09,2018-12-20T02:50:49.183611Z\n",
      "20181220_025048_0f35,0.13,2018-12-20T02:50:48.144472Z\n",
      "20181220_025047_0f35,0.07,2018-12-20T02:50:47.105333Z\n",
      "20181217_025142_101e,0.01,2018-12-17T02:51:42.296592Z\n",
      "20181217_025144_101e,0.01,2018-12-17T02:51:44.363859Z\n",
      "20181217_025143_101e,0.01,2018-12-17T02:51:43.330487Z\n",
      "20181218_031148_0f33,0.17,2018-12-18T03:11:48.793389Z\n",
      "20181218_031147_0f33,0.08,2018-12-18T03:11:47.844952Z\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.items_iter(limit=None):\n",
    "    props = item['properties']\n",
    "    stdout.write('{0},{cloud_cover},{acquired}\\n'.format(item['id'], **props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSScene4Band\n",
      "20190129_030555_1053\n",
      "204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-578e542f7d9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in item_types:\n",
    "    request = api.filters.build_search_request(query, [i])  # ,name = None, interval= 'day')\n",
    "    dataset = client.quick_search(request)\n",
    "    print(i)\n",
    "    for item in dataset.items_iter(limit=100):\n",
    "        print(item['id'])\n",
    "        session = requests.Session()\n",
    "        session.auth = (PL_API_KEY, '')\n",
    "        results = \\\n",
    "            session.get(\n",
    "                (\"https://api.planet.com/data/v1/item-types/\" +\n",
    "                 \"{}/items/{}/assets/\").format(i, item['id']))\n",
    "       \n",
    "        item_activation_url = results.json()[asset_type][\"_links\"][\"activate\"]\n",
    "       \n",
    "        response = session.post(item_activation_url)\n",
    "        print(response.status_code)\n",
    "        while response.status_code != 204:\n",
    "            time.sleep(15)\n",
    "            response = session.post(item_activation_url)\n",
    "            response.status_code = response.status_code\n",
    "            print(response.status_code)\n",
    "        \n",
    "        item_url = 'https://api.planet.com/data/v1/item-types/{}/items/{}/assets'.format(i, item['id'])\n",
    "        result = requests.get(item_url, auth=HTTPBasicAuth(PL_API_KEY, ''))\n",
    "        download_url = result.json()[asset_type]['location']  # KeyError: 'location'\n",
    "        output_file = item['id'] + '.tif'\n",
    "\n",
    "        with urllib.request.urlopen(download_url) as response, open(output_file, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_types = ['PSScene4Band']\n",
    "asset_type = 'analytic_xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"type\": \"AndFilter\",\n",
    "    \"config\": [geometry_filter, date_range_filter, cloud_cover_filter]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = filters.build_search_request(\n",
    "    query, item_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.quick_search(request)\n",
    "\n",
    "stdout.write('id,cloud_cover,date\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.items_iter(limit=None):\n",
    "    props = item['properties']\n",
    "    stdout.write('{0},{cloud_cover},{acquired}\\n'.format(item['id'], **props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in item_types:\n",
    "    request = api.filters.build_search_request(query, [i])  # ,name = None, interval= 'day')\n",
    "    dataset = client.quick_search(request)\n",
    "    print(i)\n",
    "    for item in dataset.items_iter(limit=50):\n",
    "        print(item['id'])\n",
    "        session = requests.Session()\n",
    "        session.auth = (PL_API_KEY, '')\n",
    "        results = \\\n",
    "            session.get(\n",
    "                (\"https://api.planet.com/data/v1/item-types/\" +\n",
    "                 \"{}/items/{}/assets/\").format(i, item['id']))\n",
    "       \n",
    "        item_activation_url = results.json()[asset_type][\"_links\"][\"activate\"]\n",
    "       \n",
    "        response = session.post(item_activation_url)\n",
    "        print(response.status_code)\n",
    "        while response.status_code != 204:\n",
    "            time.sleep(15)\n",
    "            response = session.post(item_activation_url)\n",
    "            response.status_code = response.status_code\n",
    "            print(response.status_code)\n",
    "        \n",
    "        item_url = 'https://api.planet.com/data/v1/item-types/{}/items/{}/assets'.format(i, item['id'])\n",
    "        result = requests.get(item_url, auth=HTTPBasicAuth(PL_API_KEY, ''))\n",
    "        download_url = result.json()[asset_type]['location']  # KeyError: 'location'\n",
    "        output_file = item['id'] + '.xml'\n",
    "\n",
    "        with urllib.request.urlopen(download_url) as response, open(output_file, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifs = []\n",
    "xmls = []\n",
    "\n",
    "for r, d, f in os.walk(os.getcwd()):\n",
    "    for file in f:\n",
    "        if file.endswith('.tif'):\n",
    "            tifs.append(os.path.join(r, file))\n",
    "        if file.endswith('.xml'):\n",
    "            xmls.append(os.path.join(r, file))\n",
    "            \n",
    "i = 0\n",
    "        \n",
    "for x in xmls:\n",
    "    with rasterio.open(tifs[i][-24:]) as src:\n",
    "            band_blue = src.read(1, masked = True)    \n",
    "    with rasterio.open(tifs[i][-24:]) as src:\n",
    "            band_green = src.read(2, masked = True)\n",
    "    with rasterio.open(tifs[i][-24:]) as src:\n",
    "            band_red = src.read(3, masked = True)\n",
    "    with rasterio.open(tifs[i][-24:]) as src:\n",
    "            band_nir = src.read(4, masked = True)\n",
    "    xmldoc = minidom.parse(xmls[i][-24:])\n",
    "    nodes = xmldoc.getElementsByTagName(\"ps:bandSpecificMetadata\")\n",
    "    \n",
    "    coeffs = {}\n",
    "    for node in nodes:\n",
    "        bn = node.getElementsByTagName(\"ps:bandNumber\")[0].firstChild.data\n",
    "        if bn in ['1', '2', '3', '4']:\n",
    "            a = int(bn)\n",
    "            value = node.getElementsByTagName('ps:reflectanceCoefficient')[0].firstChild.data\n",
    "            coeffs[a] = float(value)\n",
    "    \n",
    "    band_blue = band_blue * coeffs[1]\n",
    "    band_green = band_green * coeffs[2]\n",
    "    band_red = band_red * coeffs[3]\n",
    "    band_nir = band_nir * coeffs[4]\n",
    "    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    ndvi = ((band_nir.astype(float) - band_red.astype(float)) / (band_nir + band_red))\n",
    "    \n",
    "    kwargs = src.meta\n",
    "    kwargs.update(\n",
    "    dtype = rasterio.float32,\n",
    "    count = 1)\n",
    "    \n",
    "    with rasterio.open('ndvi_' + tifs[i][-24:], 'w', **kwargs) as dst:\n",
    "        dst.write_band(1, ndvi.astype(rasterio.float32))\n",
    "    \n",
    "        \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.mkdir('Vegetation_Indices_Imagery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp0 = os.getcwd()\n",
    "sf0 = os.listdir(sp0)\n",
    "dp0 = sp0 + ('\\\\Vegetation_Indices_Imagery')\n",
    "for file in sf0:\n",
    "    if file.startswith(('ndvi')):\n",
    "        shutil.move(os.path.join(sp0,file), os.path.join(dp0, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f for f in os.listdir(dp0) if os.path.isfile(os.path.join(dp0, f))]\n",
    "\n",
    "for image in images:\n",
    "    folder_name = image.split('_')[1]\n",
    "\n",
    "    new_path = os.path.join(dp0, folder_name)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "    old_image_path = os.path.join(dp0, image)\n",
    "    new_image_path = os.path.join(new_path, image)\n",
    "    shutil.move(old_image_path, new_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.mkdir('Original_Imagery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = os.getcwd()\n",
    "sf = os.listdir(sp)\n",
    "dp = sp + ('\\\\Original_Imagery')\n",
    "for file in sf:\n",
    "    if file.endswith(('.tif','.xml')):\n",
    "        shutil.move(os.path.join(sp,file), os.path.join(dp, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f for f in os.listdir(dp) if os.path.isfile(os.path.join(dp, f))]\n",
    "\n",
    "for image in images:\n",
    "    folder_name = image.split('_')[0]\n",
    "\n",
    "    new_path = os.path.join(dp, folder_name)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "    old_image_path = os.path.join(dp, image)\n",
    "    new_image_path = os.path.join(new_path, image)\n",
    "    shutil.move(old_image_path, new_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = []\n",
    "for r, d, f in os.walk(dp0):\n",
    "    for folder in d:\n",
    "        folders.append(os.path.join(r, folder))\n",
    "\n",
    "i = 0\n",
    "for x in folders:\n",
    "    files_to_mosaic = glob.glob(folders[i] + '/*.tif')\n",
    "    files_string = \" \".join(files_to_mosaic)\n",
    "    command = \"python \" + sp + \"\\gdal_merge.py -o \" + files_string[-24:-16] + \".tif\" + \" -of gtiff \" + files_string\n",
    "    print(command)\n",
    "    i = i + 1\n",
    "    output = subprocess.getoutput(command)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Merged_Tifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = os.getcwd()\n",
    "sf2 = os.listdir(sp2)\n",
    "dp2 = sp2 + ('\\\\Merged_Tifs')\n",
    "for file in sf2:\n",
    "    if file.endswith('.tif'):\n",
    "        shutil.move(os.path.join(sp2,file), os.path.join(dp2, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sp2 + '\\\\Merged_Tifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "for r, d, f in os.walk(dp2):\n",
    "    for file in f:\n",
    "        files.append(os.path.join(r, file))\n",
    "        \n",
    "timeavg = []\n",
    "ndviavg = []\n",
    "\n",
    "i = 0\n",
    "for x in files:\n",
    "    with rasterio.open(files[i]) as src:\n",
    "            ndvi = src.read(1, masked = True)\n",
    "    \n",
    "    ndvi2 = np.ma.masked_where(ndvi < 0.01, ndvi)\n",
    "    \n",
    "    fig2 = plt.figure(figsize=(10,10))\n",
    "    ax = fig2.add_subplot(111)\n",
    "    \n",
    "    plt.title(f[i] + \" NDVI Histogram\", fontsize=18, fontweight='bold')\n",
    "    plt.xlabel(\"NDVI values\", fontsize=14)\n",
    "    plt.ylabel(\"# pixels\", fontsize=14)\n",
    "    \n",
    "    b = ndvi2[~np.isnan(ndvi2)]\n",
    "    numBins = 20\n",
    "    ax.hist(b,numBins,color='blue',alpha=0.8)\n",
    "    ax.set_xlim([-1, 1])\n",
    "    fig2.savefig(\"ndvi_\" + f[i][0:8] + \"_histogram.png\", dpi=200, bbox_inches='tight', pad_inches=0.7)\n",
    "\n",
    "    \n",
    "    ndvi_adjusted = ndvi * (-255.0)\n",
    "    masked_ndvi_adjusted = np.ma.array(ndvi_adjusted, mask = (ndvi_adjusted == 0.))\n",
    "    cmap = plt.cm.RdYlGn\n",
    "    cmap.set_bad(color = 'white',alpha = 0.0)\n",
    "    img = masked_ndvi_adjusted.astype(np.uint8)\n",
    "    plt.imsave(\"ndvi_\" + f[i][0:8] + \"_colormap.tiff\", img, cmap=cmap)\n",
    "    plt.close()\n",
    "    \n",
    "    avg = np.average(b)\n",
    "    ndviavg.append(avg)\n",
    "    dates = f[i][4:8]\n",
    "    timeavg.append(dates)\n",
    "       \n",
    "    i = i + 1\n",
    "    \n",
    "fig3 = plt.figure(figsize=(10,10))\n",
    "ax2 = fig3.add_subplot(111)\n",
    "ax2.set_ylim([0, 1])\n",
    "plt.plot(timeavg, ndviavg, color = 'blue', linewidth = 4)\n",
    "plt.axhspan(0, 0.3, facecolor='r', alpha = 0.5, label = 'Unhealthy')\n",
    "plt.axhspan(0.3, 0.6, facecolor='y', alpha = 0.5, label = 'Somewhat Healthy')\n",
    "plt.axhspan(0.6, 1, facecolor='g', alpha = 0.5, label = 'Healthy')\n",
    "plt.legend()\n",
    "plt.title(\"Average NDVI Over Time\", fontsize=18, fontweight='bold')\n",
    "plt.xlabel(\"Start Date & End Date\", fontsize=14)\n",
    "plt.ylabel(\"Average NDVI\", fontsize=14)\n",
    "fig3.savefig(\"AverageNDVI_OverTime.png\", dpi=200, bbox_inches='tight', pad_inches=0.7)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp3 = os.getcwd()\n",
    "sf3 = os.listdir(sp3)\n",
    "\n",
    "for file in sf3:\n",
    "    if file.startswith('ndvi'):\n",
    "        shutil.move(os.path.join(sp3,file), os.path.join(sp2,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sp2)\n",
    "os.mkdir('Vegetation_Indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp4 = os.getcwd()\n",
    "sf4 = os.listdir(sp4)\n",
    "dp4 = sp2 + '\\\\Vegetation_Indices'\n",
    "\n",
    "for file in sf3:\n",
    "    if file.startswith('ndvi'):\n",
    "        shutil.move(os.path.join(sp4,file), os.path.join(dp4,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Merged_Tifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp3 = os.getcwd()\n",
    "sf3 = os.listdir(sp3)\n",
    "\n",
    "for file in sf3:\n",
    "    if file.startswith('Ave'):\n",
    "        shutil.move(os.path.join(sp3,file), os.path.join(sp2,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sp2)\n",
    "os.mkdir('Average_NDVI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp4 = os.getcwd()\n",
    "sf4 = os.listdir(sp4)\n",
    "dp4 = sp2 + '\\\\Average_NDVI'\n",
    "\n",
    "for file in sf3:\n",
    "    if file.startswith('Ave'):\n",
    "        shutil.move(os.path.join(sp4,file), os.path.join(dp4,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_types = ['PSScene3Band']\n",
    "asset_type = 'visual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"type\": \"AndFilter\",\n",
    "    \"config\": [geometry_filter, date_range_filter, cloud_cover_filter]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = filters.build_search_request(\n",
    "    query, item_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.quick_search(request)\n",
    "\n",
    "stdout.write('id,cloud_cover,date\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.items_iter(limit=None):\n",
    "    props = item['properties']\n",
    "    stdout.write('{0},{cloud_cover},{acquired}\\n'.format(item['id'], **props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in item_types:\n",
    "    request = api.filters.build_search_request(query, [i])  # ,name = None, interval= 'day')\n",
    "    dataset = client.quick_search(request)\n",
    "    print(i)\n",
    "    for item in dataset.items_iter(limit=100):\n",
    "        print(item['id'])\n",
    "        session = requests.Session()\n",
    "        session.auth = (PL_API_KEY, '')\n",
    "        results = \\\n",
    "            session.get(\n",
    "                (\"https://api.planet.com/data/v1/item-types/\" +\n",
    "                 \"{}/items/{}/assets/\").format(i, item['id']))\n",
    "       \n",
    "        item_activation_url = results.json()[asset_type][\"_links\"][\"activate\"]\n",
    "       \n",
    "        response = session.post(item_activation_url)\n",
    "        print(response.status_code)\n",
    "        while response.status_code != 204:\n",
    "            time.sleep(15)\n",
    "            response = session.post(item_activation_url)\n",
    "            response.status_code = response.status_code\n",
    "            print(response.status_code)\n",
    "        \n",
    "        item_url = 'https://api.planet.com/data/v1/item-types/{}/items/{}/assets'.format(i, item['id'])\n",
    "        result = requests.get(item_url, auth=HTTPBasicAuth(PL_API_KEY, ''))\n",
    "        download_url = result.json()[asset_type]['location']  # KeyError: 'location'\n",
    "        output_file = item['id'] + '.tif'\n",
    "\n",
    "        with urllib.request.urlopen(download_url) as response, open(output_file, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Original_Ash_Imagery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp5 = os.getcwd()\n",
    "sf5 = os.listdir(sp5)\n",
    "dp5 = sp5 + ('\\\\Original_Ash_Imagery')\n",
    "for file in sf5:\n",
    "    if file.endswith('.tif'):\n",
    "        shutil.move(os.path.join(sp5,file), os.path.join(dp5, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f for f in os.listdir(dp5) if os.path.isfile(os.path.join(dp5, f))]\n",
    "\n",
    "for image in images:\n",
    "    folder_name = image.split('_')[0]\n",
    "\n",
    "    new_path = os.path.join(dp5, folder_name)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "    old_image_path = os.path.join(dp5, image)\n",
    "    new_image_path = os.path.join(new_path, image)\n",
    "    shutil.move(old_image_path, new_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = []\n",
    "for r, d, f in os.walk(dp5):\n",
    "    for folder in d:\n",
    "        folders.append(os.path.join(r, folder))\n",
    "\n",
    "i = 0\n",
    "for x in folders:\n",
    "    files_to_mosaic = glob.glob(folders[i] + '/*.tif')\n",
    "    files_string = \" \".join(files_to_mosaic)\n",
    "    command = \"python \" + sp5 + \"\\gdal_merge.py -o \"+ files_string[-33:-25] + \".tif\" + \" -of gtiff \" + files_string\n",
    "    print(command)\n",
    "    i = i + 1\n",
    "    output = subprocess.getoutput(command)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Merged_Ash_Tifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp6 = os.getcwd()\n",
    "sf6 = os.listdir(sp5)\n",
    "dp6 = sp6 + ('\\\\Merged_Ash_Tifs')\n",
    "for file in sf6:\n",
    "    if file.endswith('.tif'):\n",
    "        shutil.move(os.path.join(sp6,file), os.path.join(dp6, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = [i for i in dir(cv2) if i.startswith('COLOR')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range1_lo = (140, 130, 105, 255)\n",
    "range1_hi = (166, 152, 129, 255)\n",
    "\n",
    "range2_lo = (201, 190, 175, 255)\n",
    "range2_hi = (210, 200, 190, 255)\n",
    "\n",
    "range3_lo = (121, 118, 114, 255)\n",
    "range3_hi = (124, 123, 119, 255)\n",
    "\n",
    "range4_lo = (145, 142, 130, 255)\n",
    "range4_hi = (150, 147, 135, 255)\n",
    "\n",
    "range5_lo = (165, 165, 150, 255)\n",
    "range5_hi = (195, 185, 175, 255)\n",
    "\n",
    "range6_lo = (155, 150, 140, 255)\n",
    "range6_hi = (170, 160, 148, 255)\n",
    "\n",
    "range7_lo = (127, 126, 115, 255)\n",
    "range7_hi = (135, 128, 125, 255)\n",
    "\n",
    "range8_lo = (190, 155, 130, 255)\n",
    "range8_hi = (230, 195, 165, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for r, d, f in os.walk(dp6):\n",
    "    for file in f:\n",
    "        files.append(os.path.join(r, file))\n",
    "\n",
    "ash_pixels = []\n",
    "timeavg = []\n",
    "\n",
    "i = 0\n",
    "for x in files:\n",
    "    img = cv2.imread(files[i])\n",
    "    img2 = cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA)\n",
    "    \n",
    "    mask_range1 = cv2.inRange(img2, range1_lo, range1_hi)\n",
    "    mask_range2 = cv2.inRange(img2, range2_lo, range2_hi)\n",
    "    mask_range3 = cv2.inRange(img2, range3_lo, range3_hi)\n",
    "    mask_range4 = cv2.inRange(img2, range4_lo, range4_hi)\n",
    "    mask_range5 = cv2.inRange(img2, range5_lo, range5_hi)\n",
    "    mask_range6 = cv2.inRange(img2, range6_lo, range6_hi)\n",
    "    mask_range7 = cv2.inRange(img2, range7_lo, range7_hi)\n",
    "    mask_range8 = cv2.inRange(img2, range8_lo, range8_hi)\n",
    "    \n",
    "    ash_mask = mask_range1+mask_range2+mask_range3+mask_range4+mask_range5+mask_range6+mask_range7+mask_range8\n",
    "    ash_result = cv2.bitwise_and(img2, img2, mask=ash_mask)\n",
    "    \n",
    "    img[ash_mask>0]=(0,0,255)\n",
    "    cv2.imwrite('Ash_Overlay_' + f[i] + '.tif', img)\n",
    "    \n",
    "    pixel_count = cv2.countNonZero(ash_mask)\n",
    "    ash_pixels.append(pixel_count)\n",
    "    \n",
    "    dates = f[i][4:8]\n",
    "    timeavg.append(dates)\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "fig3 = plt.figure(figsize=(10,10))\n",
    "ax2 = fig3.add_subplot(111)\n",
    "plt.plot(timeavg, ash_pixels, color = 'blue', linewidth = 4)\n",
    "plt.title(\"Ash Coverage Over Time\", fontsize=18, fontweight='bold')\n",
    "plt.xlabel(\"Start Date & End Date\", fontsize=14)\n",
    "plt.ylabel(\"# of Pixels\", fontsize=14)\n",
    "fig3.savefig(\"Ash_Coverage_OverTime.png\", dpi=200, bbox_inches='tight', pad_inches=0.7)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp7 = os.getcwd()\n",
    "sf7 = os.listdir(sp7)\n",
    "\n",
    "for file in sf7:\n",
    "    if file.startswith('Ash'):\n",
    "        shutil.move(os.path.join(sp7,file), os.path.join(sp2,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sp2)\n",
    "os.mkdir('Segmentation_Ash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp8 = os.getcwd()\n",
    "sf8 = os.listdir(sp8)\n",
    "dp8 = sp2 + '\\\\Segmentation_Ash'\n",
    "\n",
    "for file in sf8:\n",
    "    if file.startswith('Ash'):\n",
    "        shutil.move(os.path.join(sp8,file), os.path.join(dp8,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
